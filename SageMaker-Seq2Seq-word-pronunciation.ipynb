{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Pronunciation Example Using SageMaker (AWS-SDK) Seq2Seq\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Download dataset and preprocess](#Download-dataset-and-preprocess)\n",
    "3. [Training the Word Pronunciation model](#Training-the-Word-Pronunciation-model)\n",
    "4. [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our Word Pronunciation end-to-end example! In this demo, we will train an English Word Pronunciation model and will test the predictions on a few examples.\n",
    "\n",
    "SageMaker Seq2Seq algorithm is built on top of [Sockeye](https://github.com/awslabs/sockeye), a sequence-to-sequence framework for Neural Machine Translation based on MXNet. SageMaker Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization.\n",
    "\n",
    "SageMaker notebook has already provided you a sample Seq2seq that help you to build an English-Germany machine translation model based on a language data provided by [the Machine Translation Group at UEDIN](http://data.statmt.org/wmt17/translation-task/preprocessed/) (e.g. sample-notebooks/introduction_to_amazon_algorithms/seq2seq_translation_en-de). In this example, we are going to use Word-Pronunciation dataset provided by [CMUSphinx](https://cmusphinx.github.io/). \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. **This should be within the same region as the Notebook Instance, training, and hosting.**\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp in the cell below with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "# S3 bucket and prefix\n",
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'seq2seq/word-pronunciation'  \n",
    "# i.e.'<your_s3_bucket>/seq2seq/word-pronunciation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# For plotting attention matrix later on\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and preprocess\n",
    "\n",
    "The most of the preprocessing script is borrowed from \n",
    "https://github.com/sunilmallya/dl-twitch-series/blob/master/E2_word_pronounciations.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a word-pronunciation model on a dataset from the\n",
    "[CMUdict  --  Major Version: 0.07](http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmudict-0.7b\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "def download_data(url, force_download=True): \n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if force_download or not os.path.exists(fname):\n",
    "        urllib.request.urlretrieve(url, fname)\n",
    "    return fname\n",
    "\n",
    "url_ds1 = \"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\"\n",
    "fname = download_data(url_ds1) \n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-02-15 23:18:07--  http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\n",
      "Resolving svn.code.sf.net (svn.code.sf.net)... 216.34.181.157\n",
      "Connecting to svn.code.sf.net (svn.code.sf.net)|216.34.181.157|:80... failed: Connection refused.\n"
     ]
    }
   ],
   "source": [
    "#!wget http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = open(fname, mode = 'rt', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate words list and phones list\n",
    "words = []\n",
    "phones = []\n",
    "\n",
    "def f_char(word):\n",
    "    for c in [\"(\", \".\", \"'\", \")\", \"-\", \"_\", \"\\xc0\", \"\\xc9\", ';']: ### added ;\n",
    "        #print c in word, type(word)\n",
    "        if c in word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "i_count = 0\n",
    "\n",
    "for d in data:\n",
    "    parts = d.strip('\\n').split('  ') \n",
    "    #print(i_count)\n",
    "    #i_count += 1\n",
    "    #if not f_char(parts[0]):\n",
    "    if re.match('^[A-Z]', parts[0]) and not f_char(parts[0]):\n",
    "        words.append(parts[0])\n",
    "        phones.append(parts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a word-phoneme pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACKNOWLEDGEMENT\n",
      "AE0 K N AA1 L IH0 JH M AH0 N T\n"
     ]
    }
   ],
   "source": [
    "idx = 648\n",
    "print(words[idx])\n",
    "print(phones[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116519, 116519)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the set of charactors in the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q', 'ZH', 'AW2', 'ER1', 'DH', '0', 'EH0', 'OY2', 'U', 'IH0', 'UH0', 'UW1', 'EY2', 'F', 'AO0', 'V', 'AY1', 'G', 'I', 'O', 'EY0', 'OW1', 'X', '9', 'AA1', '8', 'EH1', 'Z', 'J', 'CH', 'M', 'E', 'IY0', 'P', '5', 'H', 'D', 'A', 'TH', 'AO2', '6', '4', 'UH2', 'AE1', 'OW2', 'AH0', 'ER0', '7', 'AH1', 'AA2', 'SH', 'JH', 'IY2', 'AA0', 'R', 'UH1', 'AY2', 'UW2', 'IY1', 'IH1', 'OY0', 'AH2', 'B', '1', 'T', 'IH2', '3', 'AW0', 'OY1', 'AW1', 'N', 'S', 'AO1', 'EY1', 'C', 'AE2', 'HH', 'W', 'AE0', 'UW0', 'EH2', 'OW0', 'K', '2', 'L', 'AY0', 'Y', 'NG', 'ER2'}\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "all_chars = set()\n",
    "for word, phone in zip(words, phones):\n",
    "    for c in word:\n",
    "        all_chars.add(c)\n",
    "    for p in phone.split(\" \"):\n",
    "        all_chars.add(p)\n",
    "        \n",
    "print(all_chars)\n",
    "print(len(all_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lets define some helper functions to convert words to symbols and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'C', 'CH', 'D', 'DH', 'E', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'H', 'HH', 'I', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'J', 'JH', 'K', 'L', 'M', 'N', 'NG', 'O', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'Q', 'R', 'S', 'SH', 'T', 'TH', 'U', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'X', 'Y', 'Z', 'ZH']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# Create a map of symbols to numbers\n",
    "symbol_set = sorted(list(all_chars))\n",
    "\n",
    "# word to symbol index\n",
    "def word_to_symbol_index(word):\n",
    "    return [symbol_set.index(char) for char in word]\n",
    "\n",
    "# list of symbol index to word\n",
    "def symbol_index_to_word(indices):\n",
    "    return [symbol_set[idx] for idx in indices]\n",
    "\n",
    "# phone to symbol index\n",
    "def phone_to_symbol_index(phone):\n",
    "    return [symbol_set.index(p) for p in phone.split(\" \")]\n",
    "\n",
    "# list of symbol index to word\n",
    "def psymbol_index_to_word(indices):\n",
    "    return [symbol_set[idx] for idx in indices]\n",
    "\n",
    "print(symbol_set)\n",
    "print(len(symbol_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 30, 57, 60, 62, 84, 58, 34, 32, 45, 34, 59, 34, 60, 74] ['A', 'C', 'K', 'N', 'O', 'W', 'L', 'E', 'D', 'G', 'E', 'M', 'E', 'N', 'T']\n"
     ]
    }
   ],
   "source": [
    "# sample word\n",
    "idx = 648\n",
    "indices_word = word_to_symbol_index(words[idx])\n",
    "print(indices_word, symbol_index_to_word(indices_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 57, 60, 12, 58, 49, 56, 59, 17, 60, 74] ['AE0', 'K', 'N', 'AA1', 'L', 'IH0', 'JH', 'M', 'AH0', 'N', 'T']\n"
     ]
    }
   ],
   "source": [
    "# sample phone\n",
    "indices_phone = phone_to_symbol_index(phones[idx])\n",
    "print(indices_phone, symbol_index_to_word(indices_phone))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any RNN task, it is important to keep track of the maximum length of input/output sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 32 34\n"
     ]
    }
   ],
   "source": [
    "# max_length\n",
    "source_sequence_length = max([len(w) for w in words])\n",
    "target_sequence_length = max([len(p.split(' ')) for p in phones])\n",
    "\n",
    "max_length = max(source_sequence_length, target_sequence_length)\n",
    "print(source_sequence_length, target_sequence_length, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put together source data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Source: Words\n",
    "dataX = []\n",
    "for word in words:\n",
    "    dataX.append(np.array(word_to_symbol_index(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10, 30, 57, 60, 62, 84, 58, 34, 32, 45, 34, 59, 34, 60, 74]),\n",
       " ['A', 'C', 'K', 'N', 'O', 'W', 'L', 'E', 'D', 'G', 'E', 'M', 'E', 'N', 'T'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 648\n",
    "dataX[idx], symbol_index_to_word(dataX[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put together target data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Target: Phonemes\n",
    "dataY =[]\n",
    "for p in phones:\n",
    "    dataY.append(np.array(phone_to_symbol_index(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14, 57, 60, 12, 58, 49, 56, 59, 17, 60, 74]),\n",
       " ['AE0', 'K', 'N', 'AA1', 'L', 'IH0', 'JH', 'M', 'AH0', 'N', 'T'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 648\n",
    "dataY[idx], symbol_index_to_word(dataY[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116519, 116519)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataY), len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC:  ['A', 'C', 'K', 'N', 'O', 'W', 'L', 'E', 'D', 'G', 'E', 'M', 'E', 'N', 'T']\n",
      "TRG:  ['AE0', 'K', 'N', 'AA1', 'L', 'IH0', 'JH', 'M', 'AH0', 'N', 'T']\n",
      "SRC:  [10 30 57 60 62 84 58 34 32 45 34 59 34 60 74]\n",
      "TRG:  [14 57 60 12 58 49 56 59 17 60 74]\n"
     ]
    }
   ],
   "source": [
    "idx = 648\n",
    "\n",
    "print(\"SRC: \", symbol_index_to_word(dataX[idx]))\n",
    "print(\"TRG: \", symbol_index_to_word(dataY[idx])) \n",
    "print(\"SRC: \", dataX[idx])\n",
    "print(\"TRG: \", dataY[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([59, 76, 71, 57, 62, 84, 72, 57, 48])\n",
      " array([44, 48, 58, 30, 46, 34, 32])\n",
      " array([69, 58, 10, 74, 34, 58, 34, 74])]\n",
      "[array([59, 38, 57, 24, 72, 57, 52]) array([44, 50, 58, 31, 74])\n",
      " array([69, 58, 42, 74, 58, 17, 74])]\n",
      "[array([63, 80, 75, 61, 66, 88, 76, 61, 52])\n",
      " array([48, 52, 62, 34, 50, 38, 36])\n",
      " array([73, 62, 14, 78, 38, 62, 38, 78])]\n",
      "[array([63, 42, 61, 28, 76, 61, 56]) array([48, 54, 62, 35, 78])\n",
      " array([73, 62, 46, 78, 62, 21, 78])]\n",
      "[array([59, 76, 71, 57, 62, 84, 72, 57, 48])\n",
      " array([44, 48, 58, 30, 46, 34, 32])\n",
      " array([69, 58, 10, 74, 34, 58, 34, 74])]\n",
      "[array([59, 38, 57, 24, 72, 57, 52]) array([44, 50, 58, 31, 74])\n",
      " array([69, 58, 42, 74, 58, 17, 74])]\n",
      "[array([63, 80, 75, 61, 66, 88, 76, 61, 52])\n",
      " array([48, 52, 62, 34, 50, 38, 36])\n",
      " array([73, 62, 14, 78, 38, 62, 38, 78])]\n",
      "[array([63, 42, 61, 28, 76, 61, 56]) array([48, 54, 62, 35, 78])\n",
      " array([73, 62, 46, 78, 62, 21, 78])]\n",
      "[array([59, 76, 71, 57, 62, 84, 72, 57, 48])\n",
      " array([44, 48, 58, 30, 46, 34, 32])\n",
      " array([69, 58, 10, 74, 34, 58, 34, 74])]\n",
      "[array([59, 38, 57, 24, 72, 57, 52]) array([44, 50, 58, 31, 74])\n",
      " array([69, 58, 42, 74, 58, 17, 74])]\n",
      "<class 'numpy.ndarray'> <class 'list'> <class 'int'>\n",
      "<class 'numpy.ndarray'> <class 'list'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "### Train Validation Split ###\n",
    "\n",
    "def shuffle_together(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "dataX, dataY = np.array(dataX), np.array(dataY)\n",
    "dataX, dataY = shuffle_together(dataX, dataY)\n",
    "\n",
    "print(dataX[:3])\n",
    "print(dataY[:3])\n",
    "\n",
    "print(dataX[:3] + 4)\n",
    "print(dataY[:3] + 4)\n",
    "\n",
    "N = int(len(dataX) * 0.9) # 90%\n",
    "\n",
    "### First 4 indices are saved for special characters ###\n",
    "\n",
    "trainX = dataX[:N] + 4\n",
    "trainY = dataY[:N] + 4\n",
    "\n",
    "print(dataX[:3])\n",
    "print(dataY[:3])\n",
    "print(trainX[:3])\n",
    "print(trainY[:3])\n",
    "\n",
    "valX = dataX[N:] + 4\n",
    "valY = dataY[N:] + 4\n",
    "\n",
    "print(dataX[:3])\n",
    "print(dataY[:3])\n",
    "\n",
    "print(type(trainX), type(trainX[0].tolist()), type(trainX[0].tolist()[0]))\n",
    "print(type(trainY), type(trainY[0].tolist()), type(trainX[0].tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vocabulary json files.\n",
    "\n",
    "Amazon SageMaker seq2seq requires two json \"vocabulary\" files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " 'A': 14,\n",
       " 'AA0': 15,\n",
       " 'AA1': 16,\n",
       " 'AA2': 17,\n",
       " 'AE0': 18,\n",
       " 'AE1': 19,\n",
       " 'AE2': 20,\n",
       " 'AH0': 21,\n",
       " 'AH1': 22,\n",
       " 'AH2': 23,\n",
       " 'AO0': 24,\n",
       " 'AO1': 25,\n",
       " 'AO2': 26,\n",
       " 'AW0': 27,\n",
       " 'AW1': 28,\n",
       " 'AW2': 29,\n",
       " 'AY0': 30,\n",
       " 'AY1': 31,\n",
       " 'AY2': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'CH': 35,\n",
       " 'D': 36,\n",
       " 'DH': 37,\n",
       " 'E': 38,\n",
       " 'EH0': 39,\n",
       " 'EH1': 40,\n",
       " 'EH2': 41,\n",
       " 'ER0': 42,\n",
       " 'ER1': 43,\n",
       " 'ER2': 44,\n",
       " 'EY0': 45,\n",
       " 'EY1': 46,\n",
       " 'EY2': 47,\n",
       " 'F': 48,\n",
       " 'G': 49,\n",
       " 'H': 50,\n",
       " 'HH': 51,\n",
       " 'I': 52,\n",
       " 'IH0': 53,\n",
       " 'IH1': 54,\n",
       " 'IH2': 55,\n",
       " 'IY0': 56,\n",
       " 'IY1': 57,\n",
       " 'IY2': 58,\n",
       " 'J': 59,\n",
       " 'JH': 60,\n",
       " 'K': 61,\n",
       " 'L': 62,\n",
       " 'M': 63,\n",
       " 'N': 64,\n",
       " 'NG': 65,\n",
       " 'O': 66,\n",
       " 'OW0': 67,\n",
       " 'OW1': 68,\n",
       " 'OW2': 69,\n",
       " 'OY0': 70,\n",
       " 'OY1': 71,\n",
       " 'OY2': 72,\n",
       " 'P': 73,\n",
       " 'Q': 74,\n",
       " 'R': 75,\n",
       " 'S': 76,\n",
       " 'SH': 77,\n",
       " 'T': 78,\n",
       " 'TH': 79,\n",
       " 'U': 80,\n",
       " 'UH0': 81,\n",
       " 'UH1': 82,\n",
       " 'UH2': 83,\n",
       " 'UW0': 84,\n",
       " 'UW1': 85,\n",
       " 'UW2': 86,\n",
       " 'V': 87,\n",
       " 'W': 88,\n",
       " 'X': 89,\n",
       " 'Y': 90,\n",
       " 'Z': 91,\n",
       " 'ZH': 92}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First 4 indices are saved for special characters ###\n",
    "vocab_dict = {c:i + 4 for i,c in enumerate(symbol_set)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 4 special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " '</s>': 3,\n",
       " '<pad>': 0,\n",
       " '<s>': 2,\n",
       " '<unk>': 1,\n",
       " 'A': 14,\n",
       " 'AA0': 15,\n",
       " 'AA1': 16,\n",
       " 'AA2': 17,\n",
       " 'AE0': 18,\n",
       " 'AE1': 19,\n",
       " 'AE2': 20,\n",
       " 'AH0': 21,\n",
       " 'AH1': 22,\n",
       " 'AH2': 23,\n",
       " 'AO0': 24,\n",
       " 'AO1': 25,\n",
       " 'AO2': 26,\n",
       " 'AW0': 27,\n",
       " 'AW1': 28,\n",
       " 'AW2': 29,\n",
       " 'AY0': 30,\n",
       " 'AY1': 31,\n",
       " 'AY2': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'CH': 35,\n",
       " 'D': 36,\n",
       " 'DH': 37,\n",
       " 'E': 38,\n",
       " 'EH0': 39,\n",
       " 'EH1': 40,\n",
       " 'EH2': 41,\n",
       " 'ER0': 42,\n",
       " 'ER1': 43,\n",
       " 'ER2': 44,\n",
       " 'EY0': 45,\n",
       " 'EY1': 46,\n",
       " 'EY2': 47,\n",
       " 'F': 48,\n",
       " 'G': 49,\n",
       " 'H': 50,\n",
       " 'HH': 51,\n",
       " 'I': 52,\n",
       " 'IH0': 53,\n",
       " 'IH1': 54,\n",
       " 'IH2': 55,\n",
       " 'IY0': 56,\n",
       " 'IY1': 57,\n",
       " 'IY2': 58,\n",
       " 'J': 59,\n",
       " 'JH': 60,\n",
       " 'K': 61,\n",
       " 'L': 62,\n",
       " 'M': 63,\n",
       " 'N': 64,\n",
       " 'NG': 65,\n",
       " 'O': 66,\n",
       " 'OW0': 67,\n",
       " 'OW1': 68,\n",
       " 'OW2': 69,\n",
       " 'OY0': 70,\n",
       " 'OY1': 71,\n",
       " 'OY2': 72,\n",
       " 'P': 73,\n",
       " 'Q': 74,\n",
       " 'R': 75,\n",
       " 'S': 76,\n",
       " 'SH': 77,\n",
       " 'T': 78,\n",
       " 'TH': 79,\n",
       " 'U': 80,\n",
       " 'UH0': 81,\n",
       " 'UH1': 82,\n",
       " 'UH2': 83,\n",
       " 'UW0': 84,\n",
       " 'UW1': 85,\n",
       " 'UW2': 86,\n",
       " 'V': 87,\n",
       " 'W': 88,\n",
       " 'X': 89,\n",
       " 'Y': 90,\n",
       " 'Z': 91,\n",
       " 'ZH': 92}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_SYMBOL = \"<pad>\" #0\n",
    "UNK_SYMBOL = \"<unk>\" #1\n",
    "BOS_SYMBOL = \"<s>\" #2\n",
    "EOS_SYMBOL = \"</s>\" #3\n",
    "\n",
    "VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
    "vocab_dict[PAD_SYMBOL] = 0\n",
    "vocab_dict[UNK_SYMBOL] = 1\n",
    "vocab_dict[BOS_SYMBOL] = 2\n",
    "vocab_dict[EOS_SYMBOL] = 3\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, source and target data share the same vocabulary dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.src.json', 'w') as fp:\n",
    "    json.dump(vocab_dict, fp, indent=4, ensure_ascii=False)\n",
    "        \n",
    "with open('vocab.trg.json', 'w') as fp:\n",
    "    json.dump(vocab_dict, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recordio-protobuf files.\n",
    "\n",
    "Amazon SageMaker expects data in the recordio-protobuf format (e.g. train.rec and val.rec). The function ``write_to_file`` generates a recordio-protobuf file from a stack of sequences using several helper functions from ``create_vocab_proto.py`` and ``record_pb2.py``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "import logging\n",
    "\n",
    "from typing import List \n",
    "from record_pb2 import Record ### record_pb2.py\n",
    "from create_vocab_proto import write_worker, write_recordio, list_to_record_bytes, read_worker\n",
    "import struct\n",
    "import io\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "            \n",
    "def write_to_file(np_dataX, np_dataY, file_type, output_file):\n",
    "    num_read_workers = max(multiprocessing.cpu_count() - 1, 1) \n",
    "    logger.info('Spawning %s encoding worker(s) for encoding %s datasets!', str(num_read_workers), file_type) \n",
    "    \n",
    "    q_in = [multiprocessing.Queue() for i in range(num_read_workers)] \n",
    " \n",
    "    q_out = multiprocessing.Queue() \n",
    "\n",
    "    read_process = [multiprocessing.Process(target=read_worker,\n",
    "                    args=(q_in[i], q_out)) for i in range(num_read_workers)] \n",
    "   \n",
    "    for p in read_process: \n",
    "        p.start()\n",
    "\n",
    "    write_process = multiprocessing.Process(target=write_worker, args=(q_out, output_file)) \n",
    "    write_process.start() \n",
    "    \n",
    "    lines_ignored = 0 # No ignored lines in this example. \n",
    "    lines_processed = 0\n",
    "    \n",
    "    for i, int_source  in enumerate(np_dataX):\n",
    "        int_source = int_source.tolist()\n",
    "        int_target = np_dataY[i].tolist()\n",
    "        item = (int_source, int_target) ### <class 'list'>, <class 'list'>\n",
    "\n",
    "        if random.random() < 0.0001:\n",
    "            ### Print some SRC-TRG pairs. \n",
    "            print('===   ===   ===   ===   ===')\n",
    "            print('SRC:', int_source)\n",
    "            print(len(int_source), type(int_source), type(int_source[0])) # num <class 'list'> <class 'int'>\n",
    "            print('---   ---   ---   ---   ---')\n",
    "            print('TRG:', int_target)\n",
    "            print(len(int_target), type(int_target), type(int_target[0])) # num <class 'list'> <class 'int'>\n",
    "\n",
    "        q_in[lines_processed % len(q_in)].put(item) \n",
    "\n",
    "        lines_processed += 1 \n",
    "    \n",
    "    logger.info(\"\"\"Processed %s lines for encoding to protobuf. %s lines were ignored as they didn't have\n",
    "                any content in either the source or the target file!\"\"\", lines_processed, lines_ignored)\n",
    "    \n",
    "    logger.info('Completed writing the encoding queue!')\n",
    "\n",
    "    for q in q_in: \n",
    "        q.put(None) \n",
    "    for p in read_process: \n",
    "        p.join()\n",
    "    logger.info('Encoding finished! Writing records to \"%s\"', output_file)\n",
    "    q_out.put(None) \n",
    "    write_process.join() \n",
    "    logger.info('Processed input and saved to \"%s\"', output_file)\n",
    "    print('+++---+++---+++---+++---+++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===   ===   ===   ===   ===\n",
      "SRC: [63, 38, 52, 76, 64, 38, 75]\n",
      "7 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [63, 31, 76, 64, 42]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [38, 78, 50, 64, 52, 34, 52, 78, 90]\n",
      "9 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [39, 79, 64, 54, 76, 53, 78, 56]\n",
      "8 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 33, 14, 75, 75, 66]\n",
      "6 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [76, 33, 16, 75, 67]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [33, 38, 36, 64, 66, 75, 91]\n",
      "7 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [33, 40, 36, 64, 24, 75, 91]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [34, 75, 66, 88, 64, 62, 52, 61, 38]\n",
      "9 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [61, 75, 28, 64, 62, 32, 61]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [78, 75, 14, 34, 61]\n",
      "5 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [78, 75, 19, 61]\n",
      "4 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 73, 38, 36]\n",
      "4 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [76, 73, 40, 36]\n",
      "4 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [63, 34, 34, 62, 52, 63, 14, 64, 76]\n",
      "9 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [63, 21, 61, 62, 31, 63, 21, 64, 91]\n",
      "9 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [38, 36, 76, 38, 62]\n",
      "5 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [40, 36, 76, 21, 62]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [33, 14, 34, 61, 76, 38, 14, 78]\n",
      "8 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [33, 18, 61, 76, 57, 78]\n",
      "6 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [75, 66, 73, 73, 66, 62, 66]\n",
      "7 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [75, 67, 73, 68, 62, 67]\n",
      "6 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 80, 63, 63, 38, 75, 48, 52, 38, 62, 36]\n",
      "11 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [76, 22, 63, 42, 48, 58, 62, 36]\n",
      "8 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 34, 14, 75, 34, 38, 62, 90]\n",
      "8 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [76, 61, 40, 75, 76, 62, 56]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [75, 52, 34, 66, 34, 50, 38, 78, 38, 36]\n",
      "10 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [75, 54, 61, 21, 77, 47, 36]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [38, 89, 73, 38, 64, 36, 14, 33, 62, 38]\n",
      "10 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [53, 61, 76, 73, 40, 64, 36, 21, 33, 21, 62]\n",
      "11 <class 'list'> <class 'int'>\n",
      "+++---+++---+++---+++---+++\n"
     ]
    }
   ],
   "source": [
    "file_type = 'train'\n",
    "output_file = \"train.rec\"\n",
    "write_to_file(trainX, trainY, file_type, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 66, 75, 78, 66, 75]\n",
      "6 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [76, 25, 75, 78, 42]\n",
      "5 <class 'list'> <class 'int'>\n",
      "+++---+++---+++---+++---+++\n"
     ]
    }
   ],
   "source": [
    "file_type = 'validation'\n",
    "output_file = \"val.rec\"\n",
    "write_to_file(valX, valY, file_type, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Upload the files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have the following 4 files. \n",
    "- train.rec : Contains source and target sequences for training in protobuf format\n",
    "- val.rec : Contains source and target sequences for validation in protobuf format\n",
    "- vocab.src.json : Vocabulary mapping (string to int) for source \n",
    "- vocab.trg.json : Vocabulary mapping (string to int) for target \n",
    "\n",
    "Let's upload the pre-processed dataset and vocabularies to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, prefix, channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = prefix + \"/\" + channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "upload_to_s3(bucket, prefix, 'train', 'train.rec') \n",
    "#/<your s3 bucket>/seq2seq/word-pronunciation/train/train.rec\n",
    "upload_to_s3(bucket, prefix, 'validation', 'val.rec') \n",
    "#/<your s3 bucket>/seq2seq/word-pronunciation/validation/val.rec \n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.src.json') \n",
    "#/<your s3 bucket>/seq2seq/word-pronunciation/vocab/vocab.src.json\n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.trg.json') \n",
    "#/<your s3 bucket>/seq2seq/word-pronunciation/vocab/vocab.trg.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those files are uploaded to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container\n",
    "\n",
    "This is where the magic happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker Seq2Seq container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/seq2seq:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/seq2seq:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/seq2seq:latest'}\n",
    "container = containers[region_name]\n",
    "print('Using SageMaker Seq2Seq container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word Pronunciation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job seq2seq-wrd-phn-p2-xlarge-2018-02-14-17-47\n",
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "job_name = 'seq2seq-wrd-phn-p2-xlarge-' + strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        # Seq2Seq does not support multiple machines. Currently, it only supports single machine, multiple GPUs\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\", # We suggest one of [\"ml.p2.16xlarge\", \"ml.p2.8xlarge\", \"ml.p2.xlarge\"]\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        # Please refer to the documentation for complete list of parameters\n",
    "        \"max_seq_len_source\": str(source_sequence_length),\n",
    "        \"max_seq_len_target\": str(target_sequence_length),\n",
    "        \"optimized_metric\": \"bleu\", \n",
    "        \"batch_size\": \"64\", # Please use a larger batch size (256 or 512) if using ml.p2.8xlarge or ml.p2.16xlarge\n",
    "        \"checkpoint_frequency_num_batches\": \"1000\",\n",
    "        \"rnn_num_hidden\": \"512\",\n",
    "        \"num_layers_encoder\": \"1\",\n",
    "        \"num_layers_decoder\": \"1\",\n",
    "        \"num_embed_source\": \"512\",\n",
    "        \"num_embed_target\": \"512\",\n",
    "        \"checkpoint_threshold\": \"3\",\n",
    "        #\"max_num_batches\": \"2100\"\n",
    "        # Training will stop after 2100 iterations/batches.\n",
    "        # This is just for demo purposes. Remove the above parameter if you want a better model.\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 48 * 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"vocab\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/vocab/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "sagemaker_client.create_training_job(**create_training_params)\n",
    "\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "### Please keep on checking the status until this says \"Completed\". ###\n",
    "\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "# if the job failed, determine why\n",
    "if status == 'Failed':\n",
    "    message = sagemaker_client.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now wait for the training job to **complete** and proceed to the next step after you see model artifacts in your S3 bucket.\n",
    "> If the cell above this returns **InProgress**, you still have to wait. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++---+++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means pronouncing word(s).\n",
    "This section involves several steps,\n",
    "- Create model - Create a model using the artifact (model.tar.gz) produced by training\n",
    "- Create Endpoint Configuration - Create a configuration defining an endpoint, using the above model\n",
    "- Create Endpoint - Use the configuration to create an inference endpoint.\n",
    "- Perform Inference - Perform inference on some input data using the endpoint.\n",
    "\n",
    "### Create model\n",
    "We now create a SageMaker Model from the training output. Using the model, we can then create an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq-wrd-phn-p2-xlarge-2018-02-14-17-47\n",
      "s3://use-nv-sagemaker/seq2seq/word-pronunciation/seq2seq-wrd-phn-p2-xlarge-2018-02-14-17-47/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-1:148886336128:model/seq2seq-wrd-phn-p2-xlarge-2018-02-14-17-47\n",
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sage = boto3.client('sagemaker')\n",
    "\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_name=job_name\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(model_name)\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Use the model to create an endpoint configuration. The endpoint configuration also contains information about the type and number of EC2 instances to use when hosting the model.\n",
    "\n",
    "Since SageMaker Seq2Seq is based on Neural Nets, we could use an ml.p2.xlarge (GPU) instance, but for this example we will use a free tier eligible ml.m4.xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqEndpointConfig-2018-02-14-20-20-44\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:148886336128:endpoint-config/seq2seqendpointconfig-2018-02-14-20-20-44\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Seq2SeqEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge', #####\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, we create the endpoint that serves up model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 10-15 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqEndpoint-2018-02-14-20-21-09\n",
      "arn:aws:sagemaker:us-east-1:148886336128:endpoint/seq2seqendpoint-2018-02-14-20-21-09\n",
      "Status: Creating\n",
      "Endpoint creation ended with EndpointStatus = InService\n",
      "CPU times: user 112 ms, sys: 8 ms, total: 120 ms\n",
      "Wall time: 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Seq2SeqEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sage.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "> Endpoint creation ended with EndpointStatus = InService\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.  \n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime = boto3.client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using JSON format for inference (Suggested for a single or small number of data instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that you don't have to convert string to text using the vocabulary mapping for inference using JSON mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B C D E F G\n"
     ]
    }
   ],
   "source": [
    "# Making an input: \" \".join(list(word.upper())) \n",
    "word_infr = 'abcdefg'\n",
    "print(\" \".join(list(word_infr.upper())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'target': 'K AA1 R'}, {'target': 'K AE1 T'}, {'target': 'T EY1 P W ER2 M'}, {'target': 'T AE1 P D AH0 N S'}, {'target': 'S UW2 P ER0 K AE2 L AH0 F R AE1 JH AH0 L IH2 S T IH0 K'}, {'target': 'EH2 K S P IY0 AE2 L AH0 D OW1 SH AH0 S'}]}\n"
     ]
    }
   ],
   "source": [
    "words_infr = [\"car\",\n",
    "        \"cat\",\n",
    "        \"tapeworm\",\n",
    "        \"tapdance\",\n",
    "        \"supercalifragilistic\",\n",
    "        \"expialidocious\"]\n",
    "\n",
    "payload = {\"instances\" : []}\n",
    "for word_infr in words_infr:\n",
    "    \n",
    "    payload[\"instances\"].append({\"data\" : \" \".join(list(word_infr.upper()))})\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "response = response[\"Body\"].read().decode(\"utf-8\")\n",
    "response = json.loads(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Attention Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing `\"attention_matrix\":\"true\"` in `configuration` of the data instance will return the attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: H E I G H T \n",
      "Target: HH AY1 T\n"
     ]
    }
   ],
   "source": [
    "word_infr = 'height'\n",
    "\n",
    "payload = {\"instances\" : [{\n",
    "                            \"data\" : \" \".join(list(word_infr.upper())),\n",
    "                            \"configuration\" : {\"attention_matrix\":\"true\"}\n",
    "                          }\n",
    "                         ]}\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "response = response[\"Body\"].read().decode(\"utf-8\")\n",
    "response = json.loads(response)['predictions'][0]\n",
    "\n",
    "source = \" \".join(list(word_infr.upper()))\n",
    "target = response[\"target\"]\n",
    "attention_matrix = np.array(response[\"matrix\"])\n",
    "\n",
    "print(\"Source: %s \\nTarget: %s\" % (source, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for plotting the attentioan matrix\n",
    "def plot_matrix(attention_matrix, target, source):\n",
    "    source_tokens = source.split()\n",
    "    target_tokens = target.split()\n",
    "    assert attention_matrix.shape[0] == len(target_tokens)\n",
    "    plt.imshow(attention_matrix.transpose(), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.xlabel(\"target\")\n",
    "    plt.ylabel(\"source\")\n",
    "    plt.gca().set_xticks([i for i in range(0, len(target_tokens))])\n",
    "    plt.gca().set_yticks([i for i in range(0, len(source_tokens))])\n",
    "    plt.gca().set_xticklabels(target_tokens)\n",
    "    plt.gca().set_yticklabels(source_tokens)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAAEYCAYAAADf1C28AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEC0Fh2gpOERL4b8/jzhymRCwNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8a99c5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(attention_matrix, target, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Protobuf format for inference (Suggested for efficient bulk inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the vocabulary mappings as this mode of inference accepts list of integers and returns list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import tempfile\n",
    "from record_pb2 import Record\n",
    "from create_vocab_proto import vocab_from_json, reverse_vocab, write_recordio, list_to_record_bytes, read_next\n",
    "\n",
    "source = vocab_from_json(\"vocab.src.json\")\n",
    "target = vocab_from_json(\"vocab.trg.json\")\n",
    "\n",
    "source_rev = reverse_vocab(source)\n",
    "target_rev = reverse_vocab(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_infr = [\"car\",\n",
    "        \"cat\",\n",
    "        \"tapeworm\",\n",
    "        \"tapdance\",\n",
    "        \"%\",\n",
    "        \"345\",\n",
    "        \"supercalifragilistic\",\n",
    "        \"expialidocious\",\n",
    "        \"Otorhinolaryngologist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the string to integers, followed by protobuf encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34, 14, 75], [34, 14, 78], [78, 14, 73, 38, 88, 66, 75, 63], [78, 14, 73, 36, 14, 64, 34, 38], [1], [7, 8, 9], [76, 80, 73, 38, 75, 34, 14, 62, 52, 48, 75, 14, 49, 52, 62, 52, 76, 78, 52, 34], [38, 89, 73, 52, 14, 62, 52, 36, 66, 34, 52, 66, 80, 76], [66, 78, 66, 75, 50, 52, 64, 66, 62, 14, 75, 90, 64, 49, 66, 62, 66, 49, 52, 76, 78]]\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to integers using source vocab mapping. Out-of-vocabulary strings are mapped to 1 - the mapping for <unk>\n",
    "words_infr = [[source.get(token, 1) for token in \"\".join(list(word_infr.upper()))] for word_infr in words_infr]\n",
    "print(words_infr)\n",
    "\n",
    "f = io.BytesIO()\n",
    "for word_infr in words_infr:\n",
    "    record = list_to_record_bytes(word_infr, [])\n",
    "    write_recordio(f, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "for word_infr in words_infr:\n",
    "    record = list_to_record_bytes(word_infr, [])\n",
    "    write_recordio(f, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-recordio-protobuf', \n",
    "                                   Body=f.getvalue())\n",
    "\n",
    "response = response[\"Body\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, parse the protobuf response and convert list of integers back to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parse_proto_response(received_bytes):\n",
    "    output_file = tempfile.NamedTemporaryFile()\n",
    "    output_file.write(received_bytes)\n",
    "    output_file.flush()\n",
    "    target_sentences = []\n",
    "    with open(output_file.name, 'rb') as datum:\n",
    "        next_record = True\n",
    "        while next_record:\n",
    "            next_record = read_next(datum)\n",
    "            if next_record:\n",
    "                rec = Record()\n",
    "                rec.ParseFromString(next_record)\n",
    "                target = list(rec.features[\"target\"].int32_tensor.values)\n",
    "                target_sentences.append(target)\n",
    "            else:\n",
    "                break\n",
    "    return target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K AA1 R', 'K AE1 T', 'T EY1 P W ER2 M', 'T AE1 P D AH0 N S', '', 'EH1 R F', 'S UW2 P ER0 K AE2 L AH0 F R AE1 JH AH0 L IH2 S T IH0 K', 'EH2 K S P IY0 AE2 L AH0 D OW1 SH AH0 S', 'OW2 T ER0 IH2 N AH0 L AA2 R IH0 NG G AA1 L AH0 JH IH0 S T']\n"
     ]
    }
   ],
   "source": [
    "targets = _parse_proto_response(response)\n",
    "resp = [\" \".join([target_rev.get(token, \"<unk>\") for token in phone_infr]) for\n",
    "                               phone_infr in targets]\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop / Close the Endpoint (Optional)\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
       "   'content-length': '0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 14 Feb 2018 23:08:25 GMT',\n",
       "   'x-amzn-requestid': 'c8edf7c8-aa28-4da0-9e19-ad9abfc88f93'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'c8edf7c8-aa28-4da0-9e19-ad9abfc88f93',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
